\chapter{Introduction}\label{introduction}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Context: What is Scala.js
\item
  Relevance: importance of networking for Scala.js
\item
  Motivation: Many JS APIs

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Websocket
  \item
    Comet
  \item
    WebRTC
  \end{itemize}
\item
  Motivation: Many network programing models

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Akka
  \item
    RPC (type safe)
  \item
    Steams (scalaz, akka-stream)
  \end{itemize}
\item
  Plan/Contributions
\end{itemize}

\chapter{Transport}\label{transport}

\TODO{This section, scala-js-transport library, main contribution}

\section{A Uniform Interface}\label{a-uniform-interface}

We begin our discussion by the definition of an interface for
asynchronous transports, presented in \autoref{transportInterface}. This
interface aims at \emph{transparently} modeling the different underlying
technologies, meaning that is simply delegates tasks to the actual
implementation, without adding new functionalities.

\transportInterface{Definition of the core networking interfaces.}

A \emph{Transport} can both \emph{listen} for incoming connections and
\emph{connect} to remote \emph{Transports}. Platforms limited to act
either as client or server will return a failed future for either of
these methods. In order to listen for incoming connections, the user of
a \emph{Transport} has to complete the promise returned by the listen
method with a \emph{ConnectionListener}. To keep the definition generic,
\emph{Address} is an abstract type. As we will see later, it varies
greatly from one technology to another.

\emph{ConnectionHandle} represents an opened connection. Thereby, it
supports four type of interactions: writing a message, listening for
incoming messages, closing the connection and listening for connection
closure. Similarly to \emph{Transport}, listening for incoming messages
is achieved by completing a promise of \emph{MessageListener}.

The presented \emph{Transport} and \emph{ConnectionHandle} interfaces
have several advantages compared to their alternative in other
languages, such the WebSocket interface in JavaScript. For example,
errors are not transmitted by throwing exceptions, but simply returned
as a failed future. Also, some incorrect behaviors such as writing to a
no yet opened connection, or receiving duplicate notifications for a
closed connection, are made impossible by construction. Thanks to
support of futures and promises in Scala.js, these interfaces cross
compile to both Java bytecode and JavaScript.

\section{Implementations}\label{implementations}

The scala-js-transport library contains several implementations of
\emph{Transports} for WebSocket, SockJS and WebRTC. This subsection
briefly presents the different technologies and their respective
advantages. \autoref{impl-summary} summarizes the available
\emph{Transports} for each platform and technology.

\begin{longtable}[c]{@{}lccc@{}}
\caption{Summary of the available
Transports.\label{impl-summary}}\tabularnewline
\toprule
Platform & WebSocket & SockJS & WebRTC\tabularnewline
\midrule
\endfirsthead
\toprule
Platform & WebSocket & SockJS & WebRTC\tabularnewline
\midrule
\endhead
JavaScript & client & client & client\tabularnewline
Play Framework & server & server & -\tabularnewline
Netty & both & - & -\tabularnewline
Tyrus & client & - & -\tabularnewline
\bottomrule
\end{longtable}

\subsection{WebSocket}\label{websocket}

WebSocket provides full-duplex communication over a single TCP
connection. Connection establishment begin with an HTTP request from
client to server. After the handshake is completed, the TCP connection
used for the initial HTTP request is \emph{upgraded} to change protocol,
and kept open to become the actual WebSocket connection. This mechanism
allows WebSocket to be wildly supported over different network
configurations.

WebSocket is also well supported across different platforms. Our library
provides four WebSocket \emph{Transports}, a native JavaScript client, a
Play Framework server, a Netty client/server and a Tyrus client. While
having all three Play, Netty and Tyrus might seem redundant, each of
them comes with its own advantages. Play is a complete web framework,
suitable to build every component of a web application. Play is based on
Netty, which means that for a standalone WebSocket server, using Netty
directly leads to better performances and less dependencies. Regarding
client side, the Tyrus library offers a standalone WebSocket client
which is lightweight compared to the Netty framework.

\subsection{SockJS}\label{sockjs}

SockJS is a WebSocket emulation protocol which fallbacks to different
protocols when WebSocket is not supported. Is supports a large number of
techniques to emulate the sending of messages from server to client,
such as AJAX long polling, AJAX streaming, EventSource and streaming
content by slowly loading an HTML file in an iframe. These techniques
are based on the following idea: by issuing a regular HTTP request from
client to server, and voluntarily delaying the response from the server,
the server side can decide when to release information. This allows to
emulate the sending of messages from server to client which not
supported in the traditional request-response communication model.

The scala-js-transport library provides a \emph{Transport} build on the
official SockJS JavaScript client, and a server on the Play Framework
via a community plugin \cite{play2-sockjs}. Netty developers have
scheduled SockJS support for the next major release.

\subsection{WebRTC}\label{webrtc}

WebRTC is an experimental API for peer to peer communication between web
browsers. Initially targeted at audio and video communication, WebRTC
also provides \emph{Data Channels} to communicate arbitrary data.
Contrary to WebSocket only supports TCP, WebRTC can be configures to use
either TCP, UDP or SCTP.

As opposed to WebSocket and SockJS which only need a URL to establish a
connection, WebRTC requires a \emph{signaling channel} in order to open
the peer to peer connection. The \emph{signaling channel} is not tight
to a particular technology, its only requirement is to allow a back an
forth communication between peers. This is commonly achieved by
connecting both peers via WebSocket to a server, which then acts as a
relay for the WebRTC connection establishment.

To simplify the process of relaying messages from one peer to another,
our library uses picklers for \emph{ConnectionHandle}. Concretely, when
a \emph{ConnectionHandle} object connecting node \emph{A} and \emph{B}
is sent by \emph{B} over an already established connection with
\emph{C}, the \emph{ConnectionHandle} received by \emph{C} will act as a
connection between \emph{A} and \emph{C}, hiding the fact that \emph{B}
relays messages between the two nodes.

The scala-js-transport library provides two WebRTC \emph{Transports},
\emph{WebRTCClient} and \emph{WebRTCClientFallback}. The later
implements some additional logic to detect WebRTC support, and
automatically fall back to using the signaling channel as substitute for
WebRTC if either peer does not support it.

At the time of writing, WebRTC is implemented is Chrome, Firefox and
Opera, and lakes support in Safari and Internet Explorer. The only non
browser implementations are available on the node.js platform.

\section{Wrappers}\label{wrappers}

By using \emph{Transport} interface, it is possible write programs with
an abstract communication medium. We present two \emph{Transport}
wrappers, for Akka \cite{akka} and Autowire~\cite{autowire}, which allow
to work with different model of concurrency. Because Autowire and Akka
(via \cite{scala-js-actors}) can both be used on the JVM and on
JavaScript, these wrappers can be used to build cross compiling programs
compatible with all the \emph{Transport} implementations presented in
\autoref{implementations}.

\subsection{Akka}\label{akka}

The actor model is based on asynchronous message passing between
primitive entities called actors. Featuring both location transparency
and fault tolerance via supervision, the actor model is particularly
adapted to distributed environment. Akka, a toolkit build around the
actor model for the JVM, was partly ported to Scala.js by S. Doeraene in
\cite{scala-js-actors}. The communication interface implemented in
\cite{scala-js-actors} was revisited into the \emph{Transport} wrapper
presented in \autoref{actorWrapper}.

\actorWrapper{Transport wrappers to handle connections with actors.}

The two methods \emph{acceptWithActor} and \emph{connectWithActor} use
the underlying \emph{listen} and \emph{connect} methods of the wrapped
\emph{Transport}, and create an \emph{handler} actor to handle the
connection. The semantic is as follows: the \emph{handler} actor is
given an \emph{ActorRef} in it is constructor, to which sending messages
results in sending outgoing messages thought the connection, and
messages received by the \emph{handler} actor are incoming messages
received from the connection. Furthermore, the life span of an
\emph{handler} actor is tight to life span of its connection, meaning
that the \emph{preStart} and \emph{postStop} hooks can be used to detect
the creation and the termination of the connection, and killing the
\emph{handler} actor results in closing the connection.
\autoref{yellingActor} shows an example of a simple \emph{handler} actor
which than sending back whatever it receives in uppercase.

\yellingActor{Example of a connection handling actor.}

Thanks to the picking mechanism developed in \cite{scala-js-actors}, it
is possible to sent messages of any type thought a connection, given
that implicit picklers are available for these types of messages. Out of
the box, picklers for case classes and case objects can be
macros-generated by the pickling library. In addition, an
\emph{ActorRef} pickler allows the transmission of \emph{ActorRefs}
thought a connection, making them transparently usable from the side of
the connection as if they were references to local actors.

\subsection{Autowire}\label{autowire}

Remote procedure call allow remote systems to communicate through an
interface similar to method calls. The Autowire library allows to
perform type-safe, reflection-free remote procedure calls between Scala
system. It uses macros and is agnostic of both the transport-mechanism
and the serialization library.

The scala-js-transport library offers a \emph{RpcWrapper}, which makes
internal use of Autowire to provide remote provide call on top of any of
the available \emph{Transports}. Because the \emph{Transport} interface
communicates with \emph{Strings}, the \emph{RpcWrapper} is able to set
all the type parameters of Autowire, as well embedding the uPickle
serialization library \cite{upickle}, thus trading flexibility to reduce
boilerplate. \autoref{rpcExample} shows a complete remote procedure call
implementation on top of WebSocket.

\rpcExample{Example of remote procedure call implementation.}

The main strength of remote procedure calls are their simplicity and
type-safety. Indeed, because of how similar remote procedure calls are
to actual method calls, they require little learning for the programmer.
In addition, consistency between client and server side can be verified
at compile time, and integrated development environment functionalities
such as \emph{refactoring} and \emph{go to definition} work out of the
box. However, this simplicity also comes with some draw backs. Contrary
to the actor model which explicitly models the life span of connections,
and different failure scenarios, this is not build in when using remote
procedure calls. In order to implement fine grain error handling and
recovery mechanism on top of remote procedure calls, one would have to
work at a lower lever than the one offered by the model itself, that is
with the \emph{Transport} interface in our case.

\section{Going further}\label{going-further}

The different \emph{Transport} implementations and wrappers presented is
this section allows for several interesting combinations. Because the
scala-js-transport library is built around a central communication
interface, it is easily expendable in both directions. Any new
implementation of the \emph{Transport} interface with for a different
platform or technology would immediately be usable with all the
wrappers. Analogously, any new \emph{Transport} wrapper would
automatically be compatible with the variety of available
implementations.

All the implementations and wrappers are accompanied by integration
tests. These tests are built using the \emph{Selenium WebDriver} to
check proper behavior of the library using real web browsers. Our tests
for WebRTC use two browsers, wich can be configured to be run with two
different browsers to test their compatibility.

\chapter{Dealing with latency}\label{dealing-with-latency}

\TODO{This section, the framework, the game}

\section{Latency Compensation}\label{latency-compensation}

Working with distributed systems introduces numerous challenges compared
the development of single machine applications. Much of the complexity
comes from the communication links; limited throughput, risk of failure,
and latency all have to be taken into consideration when information is
transfered from one machine to another. Our discussion will be focused
on issues related to latency.

When talking about latency sensitive application, the first examples
coming to mind might be multiplayer video games. In order to provide a
fun and immersive experience, real-time games have to \emph{feel}
responsive, the must offer sensations and interactions similar to the
one experienced by a tennis player when he caches the ball, or of a
Formula One driver when he drives his car at full speed. . Techniques to
compensate network latency also have uses in online
communication/collaboration tools such as \emph{Google Docs}, where
remote users can work on the same document as if they where sitting next
to each other. Essentially, any application where a shared state can be
simultaneously mutated by different peers is confronted to issues
related to latency.

While little information is available about the most recent games and
collaborative applications, the literature contains some insightful
material about the theoretical aspects of latency compensation.
According to \cite{timelines2013}, the different techniques can be
divided into three categories: predictive techniques, delayed input
techniques and time-offsettings techniques.

\emph{Predictive techniques} estimate the current value of the global
state using information available locally. These techniques are
traditionally implemented using a central authoritative server which
gathers inputs from all clients, computes the value of global state, and
broadcasts this state back to all clients. It then possible to do
prediction on the client side by computing a "throwaway" state using the
latest local inputs, which is later replaced by the state provided by
the server as soon as it is received. Predictions techniques with a
centralized server managing the application state are used in most
\emph{First person shooter} games, including recent titles built with
the Source Engine \cite{source-engine}. Predictions are sometimes
limited to certain type of objects and interactions, such as in the
\emph{dead reckoning} \cite{ieee-dead-reckoning1995} technique that
estimate the current positions of moving objects based on their earlier
position, velocity and acceleration information.

\emph{Delayed input techniques} defer the execution of all actions to
allow simultaneous execution by all peers. This solution is typically
used in application where the state, (or the variations of state) is too
large to be frequently sent over the network. In this case, peers would
directly exchange the user inputs and simultaneously simulate
application with a fixed delay. Having a centralized server is not
mandatory, and peer to peer configurations might be favored because of
the reduce communication latency. Very often, the perceived latency can
be diminished by instantly emitting a purely visual or sonorous feedback
as soon as the an input is entered, but delaying the actual effects of
the action to have it executed simultaneously on all peers. The
classical \emph{Age of Empires} series uses this techniques with a fixed
delay of 500 ms, and supports up to 8 players and 1600 independently
controllable entities \cite{aoe}.

\emph{Time-offsettings techniques} add a delay in the application of
remote inputs. Different peers will then see different versions of the
application state over time. Local perception filters
\cite{local-perception-filter1998} are an example of such techniques
where the amount of delayed applied to world entities is proportional to
their distance to the peer avatar. As a result, a user can interact in
real time with entities spatially close to him, and see the interaction
at a distance \emph{as if} they where appending in real time. The most
important limitation of local perception filters is that peers avatar
have to be kept at a minimum distance from each other, and can only
interact by exchanging passive entities, such as bullets or arrows
\cite{smed06}. Indeed, passed a certain proximity threshold, the time
distortion becomes smaller than the network latency which invalidates
the model.

Each technique comes with its own advantages and disadvantages, and are
essentially making different tradeoffs between consistency and
responsiveness. Without going into further details on the different
latency compensation techniques, this introduction should give the
reader an idea of the variety of possible solutions and their respective
sophistication.

\section{A Functional Framework}\label{a-functional-framework}

We now present scala-lag-comp, a Scala framework for predictive latency
compensation. The framework cross compiles to run on both Java virtual
machines and JavaScript engines, allowing to build applications
targeting both platforms which can transparently collaborate.

By imposing a purely functional design to its users, scala-lag-comp
focuses on correctness and leaves very little room for runtime errors.
It implements predictive latency compensation in a fully distributed
fashion. As opposed to the traditional architectures for prediction
techniques, such as the one described in~\cite{source-engine}, our
framework does uses any authoritative node to hold the global state of
the application, and can therefore functions in peer to peer, without
single points of failure.

To do so, each peer runs a local simulation of the application up to the
current time, using all the information available locally. Whenever an
input is transmitted to a peer via the network, this remote input will
necessarily be slightly out of date when it arrives at destination. In
order to incorporate this out of date input into the local simulation,
the framework \emph{rolls back} the state of the simulation as it was
just before the time of emission of this remote input, and then replays
the simulation up to the current time. \autoref{stateGraph} shows this
process in action from the point of view of peer \emph{P1}. In this
example, \emph{P1} emits an input at time \emph{t2}. Then, at time
\emph{t3}, \emph{P1} receives an input from \emph{P2} which was emitted
at time \emph{t1}. At this point, the framework invalidates a branch of
the state tree, \emph{S2-S3}, and computes \emph{S2'-S3'-S4'} to take
into account both inputs.

\stateGraph{Growth of the state graph over time, from the point of view of \emph{P1}.}

By instantaneously applying local input, the application reactiveness is
not affected by the quality of the connection; a user interacts with the
application as he would if he was the only peer involved. This property
comes with the price of having short periods of inconsistencies between
the different peers. These inconsistencies last until all peers are
aware of all inputs, at which point the simulation recovers its global
unity.

By nature, this design requires a careful management of the application
state and it evolutions over time. Indeed, even a small variation
between two remote simulations can cause a divergence, and result in
out-of-sync application states. \cite{aoe}~reports out-of-sync issues as
one of the main difficulty they encountered during the development of
multiplayer features. In our case, the \emph{roll back in time}
procedure introduces another source of potential mistake. Any mutation
in a branch of the simulation that would not properly be canceled when
rolling back to a previous state would induce serious bugs, of the hard
to isolate and hard to reproduce kind.

To cope with these issues, the scala-lag-comp framework takes entirely
care of state management and imposes a functional programming style to
its users. \autoref{engineInterface} defines the unique interface
exposed by the framework: \emph{Engine}.

\engineInterface{Interface of the latency compensation framework.}

An application is entirely defined by its \emph{initialState}, a
\emph{nextState} function that given a \emph{State} and some
\emph{Actions} emitted during a time unit computer the \emph{State} at
the next time unit, and a \emph{render} function to display
\emph{States} to the users. \emph{State} objects must be immutable, and
\emph{nextState} has to be a pure function. User \emph{Inputs} are
transmitted to an \emph{Engine} via \emph{futureAct}, and
\emph{triggerRendering} should be called whenever the platform is ready
to display the current \emph{State}, at most every 1/60th of seconds.
Finally, an \emph{Engine} excepts a \emph{broadcastConnection} to
communicate with all the participating peers.

\section{Architecture and
Implementation}\label{architecture-and-implementation}

We now give a quick overview of the architecture and implementation of
the scala-lag-comp framework. The \emph{Engine} interface presented in
\autoref{a-functional-framework} is composed of two stateful components:
\emph{ClockSync} and \emph{StateLoop}. \emph{ClockSync} is responsible
for the initial attribution of peer \emph{identity}, and the
establishment of a \emph{globalTime}, synchronized among all peers.
\emph{StateLoop} stores all the peer \emph{Inputs} and is able to
predict the application for the \emph{Inputs} received so far.
\autoref{lagcompEngine} shows the interconnection between the components
of an \emph{Engine}. The \emph{triggerRendering} function of
\emph{Engine} gets the current \emph{globalTime} from the
\emph{ClockSync}, ask the \emph{StateLoop} to predict the \emph{State}
at that time, and passes the output to the user via the \emph{render}
function. Wherever an \emph{Input} is sent to the \emph{Engine} via
\emph{futureAct}, this \emph{Input} is combined with the peer
\emph{identity} to form an \emph{Action}, then couples with the
\emph{globalTime} to form an \emph{Event}. This \emph{Event} is directly
transmitted to the local \emph{StateLoop}, and sent via the connection
to the remote \emph{StateLoops}.

\lagcompEngine{Overview the architecture of the latency compensation framework.}

\subsection{ClockSync}\label{clocksync}

The first action undertaken by the \emph{ClockSync} component is to send
a \emph{Greeting} message in broadcast, and listen for other
\emph{Greetings} message during a small time window. Peer membership and
identity are determined from these messages. Each \emph{Greeting}
message contains a randomly generated number which is used to order
peers globally, and attribute them a consistent identity.

Once peers are all aware of each other, they need to agree on a
\emph{globalTime}. Ultimately, each peer holds a difference of time
\dt between it's internal clock and the globally consented clock. The
global clock is defined to be the arithmetic average of all the peer's
clock. In order to compute their \dt, each pair of peers needs exchange
their clock values. This is accomplished in a way similar to Cristian's
algorithm \cite{cristian89}. Firstly, peers send request for the clock
values of other peers. Upon receiving a response containing a time
\emph{t}, one can estimate the value of the remote clock by adding half
of the request round trip time to \emph{t}. One all peers have
estimations of the various clocks, they are able to locally compute the
average, and use it as the estimated \emph{globalTime}., To minimize the
impact of network variations, several requests are emitted between each
pair of peers, and the algorithms only retains the requests with the
shortest round trip times.

\subsection{StateLoop}\label{stateloop}

The \emph{StateLoop} component implements the heart of the prediction
algorithm: a \emph{stateAt} function which given a \emph{time}, computes
a prediction of the application \emph{State}. To do so, \emph{StateLoop}
maintains a set of user \emph{Actions} received so far, which is used to
simulate the execution of the application. \emph{Actions} are stored in
an immutable list of \emph{Events} (pair of \emph{Input} and time),
sorted by time.

Semantically, every call to the the \emph{stateAt} function implies a
recursive computation of the current state. This recursion starts from
the \emph{initialState}, and successively applies the \emph{nextState}
function with the appropriate \emph{Events} for each time unit until the
process reaches the current time.

Obviously, doing the complete recursion on every frame would be too
expansive. However, since this process uses a pure function , called
\emph{computeState} and returning a \emph{State} given a time and list
of \emph{Events} happening before that time, it can be made efficient
using memoization. Indeed, In the most common case when \emph{stateAt}
is called and no new \emph{Events} have been received since the last
call, the cache hits right away, after a single recursion step. Whenever
a remote input is received and inserted into the sorted list of
\emph{Events}, the recursion takes place up to the time at which this
newly received \emph{Event} was issued. The cache will then hit at that
point, from where \emph{nextState} is successively applies to obtain the
\emph{State} for the current time. This is how the \emph{rolls back}
mechanism illustrated in \autoref{stateGraph} is implemented in a time
efficient manner.

Regarding memory management, timing assumptions on the network allow the
use of a bounded cache. Indeed, if we consider a peer to be disconnected
when one of his messages takes more than one second to transit over the
network, it is sufficient to retain history of \emph{States} for a
period of one second. Thus, the memorization can be implemented using a
fixed size array, retraining the association between a time, a list of
\emph{Events}, and a \emph{State}. Thanks to a careful use of immutable
lists to store \emph{Events}, querying the cache can be done using
reference equality for maximum efficiency.

\section{Putting It All Together: A Real-Time Multiplayer
Game}\label{putting-it-all-together-a-real-time-multiplayer-game}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  History: Scala.js port of a JS port of a Commodore 64 game
\item
  Functional GUI with React (Hack for the JVM version)
\item
  Everything but input handler shared (but UI shouldn't...)
\item
  Functional design of gun fire (-\textgreater{} function of time!)
\item
  WebRTC with SockJS fallback
\item
  Results: 60FPS on both platforms, lag free gameplay
\item
  Results: Lag Compensation in action (Screenshots)
\end{itemize}

\chapter{Related Work}\label{related-work}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Js/NodeJs, relies on duck typing
\item
  Closure
\item
  Steam Engine/AoE/Sc2
\item
  \cite{timelines2013}
\item
  Cheating concerns
\end{itemize}

\chapter{Conclusion and Future Work}\label{conclusion-and-future-work}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Web workers
\item
  scalaz-stream/akka-stream wrappers
\item
  More utilities on top of Transport
\end{itemize}

\appendix\clearpage\addappheadtotoc

\chapter{Scala Futures and Promises}\label{scala-futures-and-promises}

\TODO{Futures} provide a nice way to reason about performing many
operations in parallelâ€“ in an efficient and non-blocking way. The idea
is simple, a Future is a sort of a placeholder object that you can
create for a result that does not yet exist. Generally, the result of
the Future is computed concurrently and can be later collected.
Composing concurrent tasks in this way tends to result in faster,
asynchronous, non-blocking parallel code.

By default, futures and promises are non-blocking, making use of
callbacks instead of typical blocking operations. To simplify the use of
callbacks both syntactically and conceptually, Scala provides
combinators such as flatMap, foreach, and filter used to compose futures
in a non-blocking way. Blocking is still possible - for cases where it
is absolutely necessary, futures can be blocked on (although this is
discouraged).

\chapter{React}\label{react}

\TODO{React} \cite{react} is a JavaScript library for building user
interfaces.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Just the UI: Lots of people use React as the V in MVC. Since React
  makes no assumptions about the rest of your technology stack, it's
  easy to try it out on a small feature in an existing project.
\item
  Virtual DOM: React uses a virtual DOM diff implementation for
  ultra-high performance. It can also render on the server using
  Node.js, no heavy browser DOM required.
\item
  Data flow: React implements one-way reactive data flow which reduces
  boilerplate and is easier to reason about than traditional data
  binding.
\end{itemize}
